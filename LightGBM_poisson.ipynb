{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","for dirname, _, filenames in os.walk('./data_sets/'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import pandas as pd\n","import numpy as np\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.max_rows', 500)\n","import seaborn as sns; sns.set()\n","import lightgbm as lgb\n","from sklearn import preprocessing, metrics\n","from sklearn.preprocessing import LabelEncoder\n","import gc\n","import os\n","from scipy.sparse import csr_matrix\n","from scipy.stats import poisson\n","from joblib import Parallel, delayed\n","from tqdm import tqdm_notebook as tqdm\n","\n","for dirname, _, filenames in os.walk('./data_sets/'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","def reduce_mem_usage(df, verbose=True):\n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    for col in df.columns: #columns毎に処理\n","        col_type = df[col].dtypes\n","        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def read_data():\n","    folder = './data_sets/'\n","    print('Reading files...')\n","    calendar = pd.read_csv(folder+'calendar.csv')\n","    calendar = reduce_mem_usage(calendar)\n","    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n","    #\n","    sell_prices = pd.read_csv(folder+'sell_prices.csv')\n","    sell_prices = reduce_mem_usage(sell_prices)\n","    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n","    #\n","    sales_train_val = pd.read_csv(folder+'train/sales_train_validation.csv')\n","    print(\n","        'Sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0], sales_train_val.shape[1]))\n","    #\n","    submission = pd.read_csv(folder+'sample_submission.csv')\n","    #\n","    return calendar, sell_prices, sales_train_val, submission"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["calendar, sell_prices, sales_train_val, submission = read_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["NUM_ITEMS = sales_train_val.shape[0]  # 30490\n","DAYS_PRED = submission.shape[1] - 1  # 28"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def encode_categorical(df, cols):\n","    for col in cols:\n","        # Leave NaN as it is.\n","        le = LabelEncoder()\n","        not_null = df[col][df[col].notnull()]\n","        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n","    #\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["calendar = encode_categorical(\n","    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",").pipe(reduce_mem_usage)\n","\n","sales_train_val = encode_categorical(\n","    sales_train_val, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",").pipe(reduce_mem_usage)\n","\n","sell_prices = encode_categorical(sell_prices, [\"item_id\", \"store_id\"]).pipe(\n","    reduce_mem_usage\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nrows = 365 * 1 * NUM_ITEMS"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sales_train_val = pd.melt(sales_train_val,\n","                                     id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n","                                     var_name = 'day', value_name = 'demand')\n","print('Melted sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0],\n","                                                                            sales_train_val.shape[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sales_train_val = sales_train_val.iloc[-nrows:,:]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# separate test dataframes\n","forecast_submission = pd.concat([submission.iloc[0:int(771120/2),:].iloc[-30490:,:],submission.iloc[int(771120/2):,:].iloc[-30490:,:]])\n","forecast_submission['id'] = forecast_submission['id'].str.replace('_.\\...._','_')\n","forecast_submission.drop_duplicates(inplace=True)\n","\n","test1_rows = [row for row in forecast_submission['id'] if 'validation' in row]\n","test2_rows = [row for row in forecast_submission['id'] if 'evaluation' in row]\n","\n","test1 = forecast_submission[forecast_submission['id'].isin(test1_rows)]\n","test2 = forecast_submission[forecast_submission['id'].isin(test2_rows)]\n","\n","# 列别名变更\n","test1.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\n","test2.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n","\n","# test2的id的'_evaluation'置换\n","#test1['id'] = test1['id'].str.replace('_validation','')\n","test2['id'] = test2['id'].str.replace('_evaluation','_validation')\n","\n","# sales_train_val信息获取\n","product = sales_train_val[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n","\n","test1 = test1.merge(product, how = 'left', on = 'id')\n","test2 = test2.merge(product, how = 'left', on = 'id')\n","\n","# test1, test2一起进行melt处理。（销售数量：demand为0）\n","test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n","                var_name = 'day', value_name = 'demand')\n","\n","test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n","                var_name = 'day', value_name = 'demand')\n","\n","# validation为了让部分和evaluation部分明白，制作part这一列，贴上test1、test2的标签。\n","sales_train_val['part'] = 'train'\n","test1['part'] = 'test1'\n","test2['part'] = 'test2'\n","\n","data = pd.concat([sales_train_val, test1, test2], axis = 0)\n","\n","del test1, test2, sales_train_val, forecast_submission\n","\n","data = data[data['part'] != 'test2']\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#calendar\n","# drop some calendar features\n","calendar.drop(['weekday', 'wday', 'month', 'year'],\n","              inplace = True, axis = 1)\n","\n","# notebook crash with the entire dataset\n","data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n","data.drop(['d', 'day'], inplace = True, axis = 1)\n","\n","del  calendar\n","gc.collect()\n","\n","#sell price\n","# get the sell price data\n","data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n","print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n","\n","del  sell_prices\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def simple_fe(data):\n","    # 生成特征\n","    for diff in [0, 1, 2]:\n","        shift = DAYS_PRED + diff\n","        data[f\"shift_t{shift}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n","            lambda x: x.shift(shift)\n","        )\n","    #\n","    for size in [7, 30, 60, 90, 180]:\n","        data[f\"rolling_std_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n","            lambda x: x.shift(DAYS_PRED).rolling(size).std()\n","        )\n","    #\n","    for size in [7, 30, 60, 90, 180]:\n","        data[f\"rolling_mean_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n","            lambda x: x.shift(DAYS_PRED).rolling(size).mean()\n","        )\n","    #\n","    data[\"rolling_skew_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n","        lambda x: x.shift(DAYS_PRED).rolling(30).skew()\n","    )\n","    data[\"rolling_kurt_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n","        lambda x: x.shift(DAYS_PRED).rolling(30).kurt()\n","    )\n","    #\n","    # price features\n","    # price的动向和特征量化（价格变化率、与过去1年的最大价格之比等）\n","    #\n","    data[\"shift_price_t1\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n","        lambda x: x.shift(1)\n","    )\n","    data[\"price_change_t1\"] = (data[\"shift_price_t1\"] - data[\"sell_price\"]) / (\n","        data[\"shift_price_t1\"]\n","    )\n","    data[\"rolling_price_max_t365\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n","        lambda x: x.shift(1).rolling(365).max()\n","    )\n","    data[\"price_change_t365\"] = (data[\"rolling_price_max_t365\"] - data[\"sell_price\"]) / (\n","        data[\"rolling_price_max_t365\"]\n","    )\n","    #\n","    data[\"rolling_price_std_t7\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n","        lambda x: x.rolling(7).std()\n","    )\n","    data[\"rolling_price_std_t30\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n","        lambda x: x.rolling(30).std()\n","    )\n","    #\n","    # time features\n","    # 日期特征\n","    dt_col = \"date\"\n","    data[dt_col] = pd.to_datetime(data[dt_col])\n","    #\n","    attrs = [\n","        \"year\",\n","        \"quarter\",\n","        \"month\",\n","        \"week\",\n","        \"day\",\n","        \"dayofweek\",\n","        \"is_year_end\",\n","        \"is_year_start\",\n","        \"is_quarter_end\",\n","        \"is_quarter_start\",\n","        \"is_month_end\",\n","        \"is_month_start\",\n","    ]\n","    #\n","    for attr in attrs:\n","        dtype = np.int16 if attr == \"year\" else np.int8\n","        data[attr] = getattr(data[dt_col].dt, attr).astype(dtype)\n","    #\n","    data[\"is_weekend\"] = data[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n","    #\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = simple_fe(data)\n","data = reduce_mem_usage(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# going to evaluate with the last 28 days\n","x_train = data[data['date'] <= '2016-03-27']\n","y_train = x_train['demand']\n","x_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n","y_val = x_val['demand']\n","test = data[(data['date'] > '2016-04-24')]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["features = [\n","    \"item_id\",\n","    \"dept_id\",\n","    \"cat_id\",\n","    \"store_id\",\n","    \"state_id\",\n","    \"event_name_1\",\n","    \"event_type_1\",\n","    \"event_name_2\",\n","    \"event_type_2\",\n","    \"snap_CA\",\n","    \"snap_TX\",\n","    \"snap_WI\",\n","    \"sell_price\",\n","    # demand features.\n","    \"shift_t28\",\n","    \"shift_t29\",\n","    \"shift_t30\",\n","    \"rolling_std_t7\",\n","    \"rolling_std_t30\",\n","    \"rolling_std_t60\",\n","    \"rolling_std_t90\",\n","    \"rolling_std_t180\",\n","    \"rolling_mean_t7\",\n","    \"rolling_mean_t30\",\n","    \"rolling_mean_t60\",\n","    \"rolling_mean_t90\",\n","    \"rolling_mean_t180\",\n","    \"rolling_skew_t30\",\n","    \"rolling_kurt_t30\",\n","    # price features\n","    \"price_change_t1\",\n","    \"price_change_t365\",\n","    \"rolling_price_std_t7\",\n","    \"rolling_price_std_t30\",\n","    # time features.\n","    \"year\",\n","    \"month\",\n","    \"week\",\n","    \"day\",\n","    \"dayofweek\",\n","    \"is_year_end\",\n","    \"is_year_start\",\n","    \"is_quarter_end\",\n","    \"is_quarter_start\",\n","    \"is_month_end\",\n","    \"is_month_start\",\n","    \"is_weekend\"\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["weight_mat = np.c_[np.identity(NUM_ITEMS).astype(np.int8),  # item :level 12\n","                   np.ones([NUM_ITEMS, 1]).astype(np.int8),  # level 1\n","                   pd.get_dummies(product.state_id.astype(str), drop_first=False).astype('int8').values,\n","                   pd.get_dummies(product.store_id.astype(str), drop_first=False).astype('int8').values,\n","                   pd.get_dummies(product.cat_id.astype(str), drop_first=False).astype('int8').values,\n","                   pd.get_dummies(product.dept_id.astype(str), drop_first=False).astype('int8').values,\n","                   pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str), drop_first=False).astype(\n","                       'int8').values,\n","                   pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str), drop_first=False).astype(\n","                       'int8').values,\n","                   pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str), drop_first=False).astype(\n","                       'int8').values,\n","                   pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str), drop_first=False).astype(\n","                       'int8').values,\n","                   pd.get_dummies(product.item_id.astype(str), drop_first=False).astype('int8').values,\n","                   pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str), drop_first=False).astype(\n","                       'int8').values\n","].T\n","\n","weight_mat_csr = csr_matrix(weight_mat)\n","del weight_mat\n","gc.collect()\n","\n","def weight_calc(data):\n","    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n","    #\n","    sales_train_val = pd.read_csv('/kaggle/input/m5-forecasting-uncertainty/sales_train_validation.csv')\n","    #\n","    d_name = ['d_' + str(i + 1) for i in range(1913)]\n","    #\n","    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n","    #\n","    # calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n","    # 1-1914的day的数列中，将不存在销售额的日子暂时设为0，将0置换为9999。然后计算minimum number\n","    df_tmp = ((sales_train_val > 0) * np.tile(np.arange(1, 1914), (weight_mat_csr.shape[0], 1)))\n","    #\n","    start_no = np.min(np.where(df_tmp == 0, 9999, df_tmp), axis=1) - 1\n","    #\n","    weight1 = np.sum((np.diff(sales_train_val, axis=1) ** 2), axis=1) / (1913 - start_no)\n","    #\n","    # calculate the sales amount for each item/level\n","    df_tmp = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n","    df_tmp['amount'] = df_tmp['demand'] * df_tmp['sell_price']\n","    df_tmp = df_tmp.groupby(['id'])['amount'].apply(np.sum).values\n","    #\n","    weight2 = weight_mat_csr * df_tmp\n","    #\n","    weight2 = weight2 / np.sum(weight2)\n","    #\n","    del sales_train_val\n","    gc.collect()\n","    #\n","    return weight1, weight2\n","\n","\n","weight1, weight2 = weight_calc(data)\n","\n","\n","def wrmsse(preds, data):\n","    # actual obserbed values\n","    y_true = np.array(data.get_label())\n","    #\n","    # number of columns\n","    num_col = len(y_true) // NUM_ITEMS\n","    #\n","    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n","    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n","    #\n","    x_name = ['pred_' + str(i) for i in range(num_col)]\n","    x_name2 = [\"act_\" + str(i) for i in range(num_col)]\n","    #\n","    train = np.array(weight_mat_csr * np.c_[reshaped_preds, reshaped_true])\n","    #\n","    score = np.sum(\n","        np.sqrt(\n","            np.mean(\n","                np.square(\n","                    train[:, :num_col] - train[:, num_col:])\n","                , axis=1) / weight1) * weight2)\n","    #\n","    return 'wrmsse', score, False\n","\n","\n","def wrmsse_simple(preds, data):\n","    # actual obserbed values\n","    y_true = np.array(data.get_label())\n","    #\n","    # number of columns\n","    num_col = len(y_true) // NUM_ITEMS\n","    #\n","    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n","    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n","    #\n","    train = np.c_[reshaped_preds, reshaped_true]\n","    #\n","    weight2_2 = weight2[:NUM_ITEMS]\n","    weight2_2 = weight2_2 / np.sum(weight2_2)\n","    #\n","    score = np.sum(\n","        np.sqrt(\n","            np.mean(\n","                np.square(\n","                    train[:, :num_col] - train[:, num_col:])\n","                , axis=1) / weight1[:NUM_ITEMS]) * weight2_2)\n","    #\n","    return 'wrmsse', score, False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["params = {\n","    'boosting_type': 'gbdt',\n","    'metric': 'custom',\n","    'objective': 'regression',\n","    'n_jobs': -1,\n","    'seed': 236,\n","    'learning_rate': 0.1,\n","    'bagging_fraction': 0.75,\n","    'bagging_freq': 10,\n","    'colsample_bytree': 0.75}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_set = lgb.Dataset(x_train[features], y_train)\n","val_set = lgb.Dataset(x_val[features], y_val)\n","\n","del x_train, y_train\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50,\n","                  valid_sets = [train_set, val_set], verbose_eval = 100, feval= wrmsse)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# predictions for the mean of the uncertainty distribution\n","y_pred = model.predict(test[features])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# treat each prediction y as the estimated mean of a poisson distribution with mean y\n","# and use ppf to obtain the corresponding estimates of the percentiles\n","ids = test['id'].unique()\n","y_pred_q = pd.DataFrame.from_dict({'id': np.repeat(ids,28), \n","                               'date': np.tile(np.arange(np.datetime64(\"2016-04-25\"),np.datetime64(\"2016-05-23\")),30490),\n","                               '0_005': poisson.ppf(0.005, y_pred),\n","                               '0_025': poisson.ppf(0.025, y_pred),\n","                               '0_165': poisson.ppf(0.165, y_pred),\n","                               '0_250': poisson.ppf(0.25, y_pred),\n","                               '0_500': poisson.ppf(0.5, y_pred),\n","                               '0_750': poisson.ppf(0.75, y_pred),\n","                               '0_835': poisson.ppf(0.835, y_pred),\n","                               '0_975': poisson.ppf(0.975, y_pred),\n","                               '0_995': poisson.ppf(0.995, y_pred)\n","                            })\n","y_pred_q['item_id'] = y_pred_q['id'].map(lambda x: '_'.join(x.split('_')[0:3]))\n","y_pred_q['dept_id'] = y_pred_q['id'].map(lambda x: '_'.join(x.split('_')[0:2]))\n","y_pred_q['cat_id'] = y_pred_q['id'].map(lambda x: '_'.join(x.split('_')[0:1]))\n","y_pred_q['store_id'] = y_pred_q['id'].map(lambda x: '_'.join(x.split('_')[3:5]))\n","y_pred_q['state_id'] = y_pred_q['id'].map(lambda x: '_'.join(x.split('_')[3:4]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def agg_series(preds12, q):\n","    # preds12 contains 30490 series at level 12 aggregate these to get the other series\n","    preds12['id'] = preds12['item_id'] + \"_\" + preds12['store_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n","    # level 11: Unit sales of product x, aggregated for each State: 9,147\n","    preds11 = preds12.groupby(['date','item_id','state_id'], as_index=False)[q].sum()\n","    preds11['id'] = preds11['state_id'] + \"_\" + preds11['item_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n","    # level 10: Unit sales of product x, aggregated for all stores/states: 3,049\n","    preds10 = preds11.groupby(['date','item_id'], as_index=False)[q].sum()\n","    preds10['id'] = preds10['item_id'] + \"_X_\" + q.replace('_','.') + \"_validation\"\n","    # level 9: Unit sales of all products, aggregated for each store and department: 70\n","    preds09 = preds12.groupby(['date', 'store_id', 'dept_id'], as_index=False)[q].sum()\n","    preds09['id'] = preds09['store_id'] + \"_\" + preds09['dept_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n","    # level 8: Unit sales of all products, aggregated for each store and category: 30\n","    preds08 = preds12.groupby(['date', 'store_id', 'cat_id'], as_index=False)[q].sum()\n","    preds08['id'] = preds08['store_id'] + \"_\" + preds08['cat_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n","    # level 7: Unit sales of all products, aggregated for each State and department: 21\n","    preds07 = preds12.groupby(['date', 'state_id', 'dept_id'], as_index=False)[q].sum()\n","    preds07['id'] = preds07['state_id'] + \"_\" + preds07['dept_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n","    # level 6: Unit sales of all products, aggregated for each State and category: 9\n","    preds06 = preds12.groupby(['date', 'state_id', 'cat_id'], as_index=False)[q].sum()\n","    preds06['id'] = preds06['state_id'] + \"_\" + preds06['cat_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n","    # level 5: Unit sales of all products, aggregated for each department: 7\n","    preds05 = preds12.groupby(['date', 'dept_id'], as_index=False)[q].sum()\n","    preds05['id'] = preds05['dept_id'] + \"_X_\" + q.replace('_','.') + \"_validation\"\n","    # level 4: Unit sales of all products, aggregated for each category: 3\n","    preds04 = preds12.groupby(['date', 'cat_id'], as_index=False)[q].sum()\n","    preds04['id'] = preds04['cat_id'] + \"_X_\" + q.replace('_','.') + \"_validation\"\n","    # level 3: Unit sales of all products, aggregated for each store: 10\n","    preds03 = preds12.groupby(['date', 'store_id'], as_index=False)[q].sum()\n","    preds03['id'] = preds03['store_id'] + \"_X_\" + q.replace('_','.') + \"_validation\"\n","    # level 2: Unit sales of all products, aggregated for each State: 3\n","    preds02 = preds12.groupby(['date', 'state_id'], as_index=False)[q].sum()\n","    preds02['id'] = preds02['state_id'] + \"_X_\" + q.replace('_','.') + \"_validation\"\n","    # level 1: Unit sales of all products, aggregated for all stores/states: 1\n","    preds01 = preds12.groupby(['date'], as_index=False)[q].sum()\n","    preds01['id'] = 'Total_X_' + q.replace('_','.') + '_validation'\n","    preds = pd.concat([preds01, preds02, preds03,\n","                                        preds04, preds05, preds06,\n","                                        preds07, preds08, preds09,\n","                                        preds10, preds11, preds12],\n","                                       ignore_index=True)\n","    return preds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# this section has been edited so we generate validation as well as test predictions\n","predictions_0_005 = agg_series(y_pred_q,'0_005')[['date', 'id', '0_005']]\n","predictions_0_005 = predictions_0_005.sort_values(by=['date', 'id'])\n","test_0_005 = predictions_0_005[-1199520:].pivot(index = 'id', columns = 'date', values = '0_005').reset_index()\n","test_0_005.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n","\n","predictions_0_025 = agg_series(y_pred_q,'0_025')[['date', 'id', '0_025']]\n","predictions_0_025 = predictions_0_025.sort_values(by=['date', 'id'])\n","test_0_025 = predictions_0_025[-1199520:].pivot(index = 'id', columns = 'date', values = '0_025').reset_index()\n","test_0_025.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n","\n","predictions_0_165 = agg_series(y_pred_q,'0_165')[['date', 'id', '0_165']]\n","predictions_0_165 = predictions_0_165.sort_values(by=['date', 'id'])\n","test_0_165 = predictions_0_165[-1199520:].pivot(index = 'id', columns = 'date', values = '0_165').reset_index()\n","test_0_165.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n","\n","predictions_0_250 = agg_series(y_pred_q,'0_250')[['date', 'id', '0_250']]\n","predictions_0_250 = predictions_0_250.sort_values(by=['date', 'id'])\n","test_0_250 = predictions_0_250[-1199520:].pivot(index = 'id', columns = 'date', values = '0_250').reset_index()\n","test_0_250.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n","\n","predictions_0_500 = agg_series(y_pred_q,'0_500')[['date', 'id', '0_500']]\n","predictions_0_500 = predictions_0_500.sort_values(by=['date', 'id'])\n","test_0_500 = predictions_0_500[-1199520:].pivot(index = 'id', columns = 'date', values = '0_500').reset_index()\n","test_0_500.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n","\n","predictions_0_750 = agg_series(y_pred_q,'0_750')[['date', 'id', '0_750']]\n","predictions_0_750 = predictions_0_750.sort_values(by=['date', 'id'])\n","test_0_750 = predictions_0_750[-1199520:].pivot(index = 'id', columns = 'date', values = '0_750').reset_index()\n","test_0_750.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n","\n","predictions_0_835 = agg_series(y_pred_q,'0_835')[['date', 'id', '0_835']]\n","predictions_0_835 = predictions_0_835.sort_values(by=['date', 'id'])\n","test_0_835 = predictions_0_835[-1199520:].pivot(index = 'id', columns = 'date', values = '0_835').reset_index()\n","test_0_835.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n","\n","predictions_0_975 = agg_series(y_pred_q,'0_975')[['date', 'id', '0_975']]\n","predictions_0_975 = predictions_0_975.sort_values(by=['date', 'id'])\n","test_0_975 = predictions_0_975[-1199520:].pivot(index = 'id', columns = 'date', values = '0_975').reset_index()\n","test_0_975.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n","\n","predictions_0_995 = agg_series(y_pred_q,'0_995')[['date', 'id', '0_995']]\n","predictions_0_995 = predictions_0_995.sort_values(by=['date', 'id'])\n","test_0_995 = predictions_0_995[-1199520:].pivot(index = 'id', columns = 'date', values = '0_995').reset_index()\n","test_0_995.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test = pd.concat([test_0_005, test_0_025, test_0_165, test_0_250, test_0_500\n","    , test_0_750, test_0_835, test_0_975, test_0_995])\n","test = pd.concat([test, test])\n","test['id'][-385560:] = test['id'][-385560:].str.replace('_validation','_evaluation')\n","test.fillna(0,inplace=True)\n","test.to_csv(\"submission.csv\",index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["################# Scaled Pinball Loss calculation #################"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# weights for the weighted SPL\n","data['amount'] = data['demand'] * data['sell_price']\n","amounts = data.sort_values(by=['id','date'])[['id','date','amount']]\n","amounts['item_id'] = amounts['id'].map(lambda x: '_'.join(x.split('_')[0:3]))\n","amounts['dept_id'] = amounts['id'].map(lambda x: '_'.join(x.split('_')[0:2]))\n","amounts['cat_id'] = amounts['id'].map(lambda x: '_'.join(x.split('_')[0:1]))\n","amounts['store_id'] = amounts['id'].map(lambda x: '_'.join(x.split('_')[3:5]))\n","amounts['state_id'] = amounts['id'].map(lambda x: '_'.join(x.split('_')[3:4]))\n","amounts = agg_series(amounts,'amount')[['date', 'id', 'amount']]\n","amounts = amounts.sort_values(by=['id','date'])\n","wgts = amounts.groupby('id').mean()['amount'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# the validation predictions\n","val = pd.concat([predictions_0_005[:1199520].rename(columns={\"0_005\": \"demand\"})\n","                ,predictions_0_025[:1199520].rename(columns={\"0_025\": \"demand\"})\n","                ,predictions_0_165[:1199520].rename(columns={\"0_165\": \"demand\"})\n","                ,predictions_0_250[:1199520].rename(columns={\"0_250\": \"demand\"})\n","                ,predictions_0_500[:1199520].rename(columns={\"0_500\": \"demand\"})\n","                ,predictions_0_750[:1199520].rename(columns={\"0_750\": \"demand\"})\n","                ,predictions_0_835[:1199520].rename(columns={\"0_835\": \"demand\"})\n","                ,predictions_0_975[:1199520].rename(columns={\"0_975\": \"demand\"})\n","                ,predictions_0_995[:1199520].rename(columns={\"0_995\": \"demand\"})])\n","val = val.sort_values(by=['id','date']) # sorted by id, q, date\n","demandval = val['demand'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# the actual values on train and val\n","Ytrainval = data[data['date'] <= '2016-04-24'][['id','date', 'demand']]\n","Ytrainval['item_id'] = Ytrainval['id'].map(lambda x: '_'.join(x.split('_')[0:3]))\n","Ytrainval['dept_id'] = Ytrainval['id'].map(lambda x: '_'.join(x.split('_')[0:2]))\n","Ytrainval['cat_id'] = Ytrainval['id'].map(lambda x: '_'.join(x.split('_')[0:1]))\n","Ytrainval['store_id'] = Ytrainval['id'].map(lambda x: '_'.join(x.split('_')[3:5]))\n","Ytrainval['state_id'] = Ytrainval['id'].map(lambda x: '_'.join(x.split('_')[3:4]))\n","Ytrainval = agg_series(Ytrainval,'demand')[['date', 'id', 'demand']]\n","Ytrainval = Ytrainval.sort_values(by=['id','date'])\n","Ytrainvaldemand = Ytrainval['demand'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# SPL for a single time series and single quantile\n","def SPL(ts,Qu,u,n,h):\n","    return (1.0/h) * (n-1) * sum((ts[n:n+h] - Qu) * u * (Qu <= ts[n:n+h]) + \\\n","                         (Qu - ts[n:n + h]) * (1-u) * (Qu > ts[n:n + h])) / \\\n","                    sum(abs(ts[1:n] - ts[0:(n-1)]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# SPL for a single time series and multiple quantiles\n","def meanSPL(ts,Q,n,h):\n","    u = [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995]\n","    #\n","    spls = np.zeros(len(u))\n","    for j in range(len(u)):\n","        spls[j] = SPL(ts,Q[j,:],u[j],n,h)\n","    #\n","    return np.mean(spls)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# val and train lengths\n","h = 28\n","n = int(Ytrainval.shape[0]/42840) - h"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# run the calcn\n","def mymeanSPL(i):\n","    Q = demandval[i*28*9:(i+1)*28*9].reshape([9,28])\n","    ts = Ytrainvaldemand[i*(n+h):(i+1)*(n+h)]\n","    return meanSPL(ts,Q,n,h)\n","\n","\n","lSPL = Parallel(n_jobs=-1)(delayed(mymeanSPL)(i) for i in tqdm(range(42840)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# massage the results\n","lSPL = np.asarray(lSPL)\n","lSPL[np.isnan(lSPL)] = 0\n","lSPL[np.isinf(lSPL)] = 0\n","wSPL = np.sum(wgts*lSPL)/np.sum(wgts)\n","print(f'Weighted Scaled Pinball Loss is: {wSPL}.')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":1236843,"sourceId":18600,"sourceType":"competition"}],"dockerImageVersionId":29860,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
